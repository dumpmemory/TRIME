# TRIME: Training Language Models with Memory Augmentation

This is the repository for the paper [Training Language Models with Memory Augmentation](https://arxiv.org/abs/2205.12674), by [Zexuan Zhong](https://www.cs.princeton.edu/~zzhong/), [Tao Lei](https://taolei87.github.io), and [Danqi Chen](https://www.cs.princeton.edu/~danqic/).

Our code and models are coming soon!

We propose a new training objective TRIME for language modeling, which aligns model outputs with both token embeddings and *in-batch memories*. We also devise novel ways for data batching and constructing training memories, so that our models can leverage *long-range contexts* and *external datastore* effectively.

<img src="images/method.png" width="600">

If you have any questions about our paper, please contact Zexuan Zhong (zzhong@cs.princeton.edu)!
