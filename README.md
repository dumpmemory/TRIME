# TRIME: Training Language Models with Memory Augmentation

This is the repository for the paper [Training Language Models with Memory Augmentation](https://github.com/princeton-nlp/TRIME/blob/main/paper/TRIME.pdf), by Zexuan Zhong, Tao Lei, and Danqi Chen.

Our code and models are coming soon!

We propose a new training objective TRIME for language modeling—inspired by contrastive learning—which aligns with both token embeddings and *in-batch memories*. 

![Method](images/method.jpg)

If you have any questions about our paper, please contact Zexuan Zhong (zzhong@cs.princeton.edu)!
