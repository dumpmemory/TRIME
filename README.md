# TRIME: Training Language Models with Memory Augmentation

This is the repository for the paper [Training Language Models with Memory Augmentation](https://www.cs.princeton.edu/~zzhong/papers/TRIME.pdf), by Zexuan Zhong, Tao Lei, and Danqi Chen.

Our code and models are coming soon!

We propose a new training objective TRIME for language modeling—inspired by contrastive learning—which aligns with both token embeddings and *in-batch memories*. 

<img src="images/method.png" width="600">

If you have any questions about our paper, please contact Zexuan Zhong (zzhong@cs.princeton.edu)!
