# TRIME: Training Language Models with Memory Augmentation

This is the repository for the paper [Training Language Models with Memory Augmentation](https://www.cs.princeton.edu/~zzhong/papers/TRIME.pdf), by Zexuan Zhong, Tao Lei, and Danqi Chen.

Our code and models are coming soon!

We propose a new training objective TRIME for language modeling, which aligns with both token embeddings and *in-batch memories*. We also devise novel ways for data batching and constructing training memories, so that our models can leverage *long-range contexts* and *external datastore* effectively.

<img src="images/method.png" width="600">

If you have any questions about our paper, please contact Zexuan Zhong (zzhong@cs.princeton.edu)!
